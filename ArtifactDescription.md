# Artifact Description

## 概要：e-desk

以下，記述事項の説明．

* 改変対象OSS:[e-desk](https://github.com/ryusuke-m/e-desk)
  + プロジェクターとカメラ(webカメラもしくは深度カメラ)を用いたプロジェクションアプリ
  + 机の上を丸ごと電子書籍のように扱えるようにすることを目的とする．
    + 具体的にはオープンソースの画像処理ライブラリ[OpenCV](https://opencv.org)標準のARマーカを用いて，机の四隅を認識し，(ARマーカの印刷された)紙面上に電子書籍コンテンツを投影する．
      + 投影する際には，ARマーカの位置(=紙の傾き)を見て，正しく投影されるように映像を射影変換する．
    + また，オープンソースの物体検出・画像分割エンジン[YOLO](https://docs.ultralytics.com/ja)を利用したハンドジェスチャーによって，コンテンツを操作する．
* 改変内容「シミュレーションモードの実装」
  + シミュレーションモードを実装
  + プロジェクターが無い状態でも起動，仮想的な動作を可能にする．
    + また，X Window System互換のシステムが実装されていない場合でもシミュレーション動作が可能．
  + あくまで動作であり，完全な動作を可能にするわけではない．
    + 動画への対応が出来ていない．
  + また，映像出力については，出力された画像をDocker上で立てたサーバから確認できる．

## クイックスタート

```
docker pull watson9109/2024-t2201411-e-desk
docker run -p -p 8000:8000 --rm watson9109/e-desk:latest 
```
以下はDockerコンテナ内での操作
### シミュレーションモードの結果を見る場合
画像表示の為にローカルにサーバを立てる必要がある．
```
python3 pythonserver.py
```
`http://localhost:8000` にアクセスすることで結果が確認できる．

### 仮想カメラとして入力する画像の作成
```
python3 GenAR.py --lenx 700 --leny 600
```
仮想カメラの入力として`VirtualDesk.py`が作成される．
なお，コマンドライン引数`lenx`, `leny`はそれぞれコンテンツが投影される紙の範囲(=紙の大きさ，向き)を表している．
上のコマンドでは，各引数を700,600と設定して，実行している．
引数はそれぞれ200から1080, 200から1920の範囲で変更可能だ．

### シミュレーションモードの再実行
```
python3 Start.py --sim SIM
```
> [!NOTE]
> シミュレーションは仮想カメラを利用すること以外通常の起動と同じ挙動をすることから，自動で終了しない．
> 以下のような出力が確認できたら，`Ctrl+c`等で終了させること．その際に出力されるエラーは無視して問題ない．

```
-----
[[6]
 [7]
 [2]
 [3]
 [5]
 [1]
 [4]
 [0]]
------
```

## 評価手順

### シミュレーションモードの結果を見る場合
* シミュレーションモードの結果として，４つの画像(仮想カメラの入力，認識した机の上の様子，ARマーカの認識，プロジェクターから映し出される映像)が確認できるか．

### 仮想カメラとして入力する画像の作成
* 仮想カメラとして入力する画像`VirtualDesk.png`が作成できるか．
* また，引数を変えて作成した画像を用いて，シミュレーションモードの再実行を行えるか
* 先のシミュレーションモードの再実行後，結果の変化が確認できるか

### シミュレーションモードの再実行
* シミュレーションモードが問題なく実行できるか
* シミュレーションモードの再実行後，結果が正しく出力されていることが確認できるか

## 制限と展望

* GUI環境での操作を前提としていた実装であった為，Dockerへの実装に際し，一部無理が出た．
  * シミュレーションモードを`Ctrl+c`で終了させる実装等がその一例．
* カメラ入力について
  * 時間等の都合でカメラ入力として，あらかじめ作成した画像を利用するような形とした．
  * Webカメラ，内蔵カメラからの映像処理は行わなかった．
    + [Docker Community Forum](https://forums.docker.com/t/how-to-use-a-host-usb-device-in-a-container-in-docker-desktop/138905)によると，WindowsもしくはOSXでは，LinuxVM下で動かす必要があるらしい．
    + 各環境によってUSBデバイスの引数が変わりうる為，簡単な評価が提供できないことから行わなかった．
  * WebRTCを用いてカメラ入力を取ってくることは時間に余裕があれば行いたかった．
    + WebRTCを用いて，ローカルに立てたサーバにHTTPSで映像の受送信を行うことも考えた．
    + TypeScriptでWebRTC関連の処理を実装する必要があり，時間が足りず，十分な動作を保証出来なかった為，実装を諦めた．
    + ネットワーク上での映像のやり取りには興味があるので，暇があればやろうと思う．
* 映像出力について
  * Dockerで映像出力を行うためには，現在ディスプレイを管理しているX Window System互換のサーバを渡す必要がある．
  * カメラ入力同様に簡単な評価が提供できないことから行わなかった．
  * WebRTCについても同様．  
* Dockerイメージの肥満(8GBもある!!)
  + OpenCV，YOLO等画像処理ライブラリ，モデルが入っている為もあり，Dockerイメージが肥えている．
  + マルチステージングや，slimイメージをベースにする，画像認識モデルをより小さくすることが必要．
* プロセスを作った後，安全に終了するようになっていない．
  + 実行の後はDockerのコンテナ丸ごと消去するだろうという考えの為
  + 本来なら安全にプロセスを終了できるようにすべきだった．(時間不足)
  + 
## 更なる使い方（オプション）

* 現在は仮想カメラからの画像，映像の処理を行うことができる．
  * エンドユーザよりも，開発時の利用が主となる．
    * 事前に撮影した映像，画像の処理を確認できる．
    * 例えば，一部処理を書き換えた際の映像処理の差異を確認できる．
* また，現在は実装出来ていないが，WebRTCを用いた仮想カメラの実装ができればエンドユーザ向けの利用も可能になる．
  * サーバへの処理の委託
    * サーバに画像認識等の重い処理を任せられるようになる為，デバイスの取り回しが良くなる．
    * プロジェクタ側はHTTPを処理でき，カメラとプロジェクタを取り付けるだけで良くなる．