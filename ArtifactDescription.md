# Artifact Description

## 概要：e-desk

以下，記述事項の説明．

* 改変対象OSS:[e-desk](https://github.com/ryusuke-m/e-desk)
  + プロジェクターとカメラ(webカメラもしくは深度カメラ)を用いたプロジェクションアプリ
  + 机の上を丸ごと電子書籍のように扱えるようにすることを目的とする．
    + 具体的にはオープンソースの画像処理ライブラリ[OpenCV](https://opencv.org)標準のARマーカを用いて，机の四隅を認識し，(ARマーカの印刷された)紙面上に電子書籍コンテンツを投影する．
      + 投影する際には，ARマーカの位置(=紙の傾き)を見て，正しく投影されるように映像を射影変換する．
    + また，オープンソースの物体検出・画像分割エンジン[YOLO](https://docs.ultralytics.com/ja)を利用したハンドジェスチャーによって，コンテンツを操作する．(今回の実装では検証出来ない．)
* 改変内容「シミュレーションモードの実装」
  + シミュレーションモードを実装
  + プロジェクターが無い状態でも起動，仮想的な動作を可能にする．
    + また，X Window System互換のシステムが実装されていない場合でもシミュレーション動作が可能．
  + あくまで動作であり，完全な動作を可能にするわけではない．
    + 動画への対応が出来ていない．
  + また，映像出力については，出力された画像をDocker上で立てたサーバから確認できる．

## クイックスタート

詳しい評価方法については[評価手順](#評価手順)に従ってほしい．

まず，適当なディレクトリ内にて，Dockerコンテナを作成する．
```
docker pull watson9109/2024-t2201411-e-desk
docker run -p 8000:8000 -it --rm watson9109/e-desk:latest 
```
以下はDockerコンテナ内での操作となる．
### シミュレーションモードの結果を見る場合
画像表示の為にローカルにサーバを立てる必要がある．
```
python3 pythonserver.py
```
http://localhost:8000 にアクセスすることで結果が確認できる．
> [!NOTE]
> サーバ終了後，再度サーバを建てようとしてもサーバが建てられないことがある．ポート8000番が前のプログラムの終了処理で埋まっている為であり，しばらく待つと再度サーバが建てられる．


### 仮想カメラとして入力する画像の作成
```
python3 GenAR.py --lenx 1080 --leny 1920
```
仮想カメラの入力として`VirtualDesk.py`が作成される．
なお，コマンドライン引数`lenx`, `leny`はそれぞれコンテンツが投影される紙の範囲(=紙の大きさ，向き)を表している．
上のコマンドでは，各引数を700,600と設定して，実行している．
引数はそれぞれ600から1080, 600から1920の範囲で変更可能だ．

### シミュレーションモードの再実行
```
python3 Start.py --sim SIM
```
シミュレーションの結果が，`VirtualEDesk.png`, `VirtualEProjector.png`, `Aruco.png`に出力される．

## 評価手順

まず，Dockerコンテナの`/e-desk`ディレクトリ内に`VirtualDesk.png`, `VirtualEDesk.png`, `VirtualEProjector.png`, `Aruco.png`の4つのファイルが存在することを確認してほしい．

次に，[クイックスタート/シミュレーションモードの結果を見る場合](#シミュレーションモードの結果を見る場合)に従ってローカルサーバを立て，現在の画像を確認する．

プログラムを`Ctrl+c`等で終了し，[クイックスタート/仮想カメラとして入力する画像の作成](#仮想カメラとして入力する画像の作成)に従って，新たな入力画像を作成する．(Dockerコンテナ内の画像は`python3 GenAR.py --lenx 1000 --leny 1000`を実行した場合の画像である為，クイックスタート内のコマンドをコピペして問題ない．)

最後に，[クイックスタート/シミュレーションモードの再実行](#シミュレーションモードの再実行)に従って，シミュレーションモードを再実行した後，先に生成した仮想カメラの画像及びその画像を用いたシミュレーション結果をサーバを立てて確認する．先に確認した画像と異なり，先に実行したコマンドの引数に従った画像が出力されているはずである．

## 制限と展望
* カメラ入力について
  * 時間等の都合でカメラ入力として，あらかじめ作成した画像を利用するような形とした．
  * Webカメラ，内蔵カメラからの映像処理は行わなかった．
    + [Docker Community Forum](https://forums.docker.com/t/how-to-use-a-host-usb-device-in-a-container-in-docker-desktop/138905)によると，WindowsもしくはOSXでは，LinuxVM下で動かす必要があるらしい．
    + 各環境によってUSBデバイスの引数が変わりうる為，簡単な評価が提供できないことから行わなかった．
  * WebRTCを用いてカメラ入力を取ってくることは時間に余裕があれば行いたかった．
    + WebRTCを用いて，ローカルに立てたサーバにHTTPSで映像の受送信を行うことも考えた．
    + TypeScriptでWebRTC関連の処理を実装する必要があり，時間が足りず，十分な動作を保証出来なかった為，実装を諦めた．
    + ネットワーク上での映像のやり取りには興味があるので，暇があればやろうと思う．
* 映像出力について
  * Dockerで映像出力を行うためには，現在ディスプレイを管理しているX Window System互換のサーバを渡す必要がある．
  * カメラ入力同様に簡単な評価が提供できないことから行わなかった．
  * WebRTCについても同様．  
* Dockerイメージの肥満(8GBもある!!)
  + OpenCV，YOLO等画像処理ライブラリ，モデルが入っている為もあり，Dockerイメージが肥えている．
  + マルチステージングや，slimイメージをベースにする，画像認識モデルをより小さくすることが必要．
# 更なる使い方（オプション）

* 現在は仮想カメラからの画像，映像の処理を行うことができる．
  * エンドユーザよりも，開発時の利用が主となる．
    * 事前に撮影した映像，画像の処理を確認できる．
      * 例えば，一部処理を書き換えた際の映像処理の差異を確認できる．
* また，現在は実装出来ていないが，WebRTCを用いた仮想カメラの実装ができればエンドユーザ向けの利用も可能になる．
  * サーバへの画像処理の委託
    * サーバに画像認識等の重い処理を任せられるようになる為，デバイスの取り回しが良くなる．
      * プロジェクタ側のコンピュータはHTTPを処理できる程度の性能があり，カメラとプロジェクタを取り付られる程度のI/Oがあればよい．